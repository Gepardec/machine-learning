{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Feed-Foward Network in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import PyTorch\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data \n",
    "data = np.loadtxt(open(\"../data/Absenteeism_at_work.csv\", \"rb\"), delimiter=\";\", skiprows=1)\n",
    "# Use all data as input with the exception of one column (and convert to float)\n",
    "input_ = torch.from_numpy(np.delete(data, 14, axis=1)).float()\n",
    "# Use the column is_social_drinker? as target for prediction (set it to the right dimention and float)\n",
    "# is_social_drinker is easier to train than is_social_smoker (15) as the data are more balanced. \n",
    "target = torch.from_numpy(data[:, 14].reshape(data.shape[0], 1)).float()\n",
    "\n",
    "# Create training and test datasets with 500 and 240 elements respectively\n",
    "subsets = torch.utils.data.random_split(target, [500, 240])\n",
    "training_input = input_[subsets[0].indices]\n",
    "training_target = target[subsets[0].indices]\n",
    "test_input = input_[subsets[1].indices]\n",
    "test_target = target[subsets[1].indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    # Constructor\n",
    "    # All elements of `self` are fields of a new object. \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # We define a net with three linear hidden layers\n",
    "        self.fc1 = nn.Linear(20, 20)\n",
    "        self.fc2 = nn.Linear(20, 20)\n",
    "        self.fc3 = nn.Linear(20, 1)\n",
    "\n",
    "    # PyTorch is clever enough to automatically generate `backward()`\n",
    "    def forward(self, x):\n",
    "        # with ReLU activation functions\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # Sigmoid as activation in the last layer\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new net with randomly initialised parameters (weight matrices)\n",
    "net = Net()\n",
    "# The parameters (weight matrices) are randomly initialised\n",
    "#list(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def training(learning_rate) :\n",
    "    # Calculate the output based on current parameters\n",
    "    prediction = net(training_input)\n",
    "    # Calculate loss\n",
    "    loss = F.binary_cross_entropy(prediction, training_target)\n",
    "    # Back-propagate\n",
    "    net.zero_grad()\n",
    "    loss.backward()\n",
    "    # Update the weight matrices\n",
    "    for f in net.parameters():\n",
    "        f.data.sub_(f.grad.data * learning_rate)\n",
    "    return loss\n",
    "\n",
    "def accuracy(input_, target):\n",
    "    prediction = net(input_)\n",
    "    result = (prediction > 0.5) == target.type(torch.ByteTensor)\n",
    "    return sum(result.type(torch.FloatTensor)) / len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The untrained network generates random predictions and its accuracy is about 50% for balanced dataset,\n",
    "# or (1 - ) sum(target) / len(target), respectively.\n",
    "accuracy(input_, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate must be < 1, small enough for the parameters to converge and large enough for an efficient training.\n",
    "# Vary this value to see how the training evolves.\n",
    "learning_rate = 0.001 \n",
    "# Number of iterations.\n",
    "n_iterations = 10000\n",
    "for i in range(0, n_iterations):\n",
    "    result = training(learning_rate)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(training_input, training_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(test_input, test_target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
